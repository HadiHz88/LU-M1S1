{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d857b616",
   "metadata": {},
   "source": [
    "# 1. Calculus Essentials: Derivatives & Gradients\n",
    "\n",
    "In Machine Learning, we optimize a model by minimizing a **Loss Function** $J(w)$.\n",
    "To do this, we need to know which direction to move the weights.\n",
    "\n",
    "- **Derivative ($f'(x)$):** Measures the slope of a function for a single variable.\n",
    "- **Gradient ($\\nabla f(x)$):** A vector of partial derivatives for multiple variables. It points in the direction of steepest **ascent** (uphill).\n",
    "\n",
    "$$\\nabla f(x) = \\left( \\frac{\\partial f}{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_d} \\right)^T$$\n",
    "\n",
    "**The Golden Rule:** To minimize loss, we move in the **opposite** direction of the gradient:\n",
    "$$w_{new} = w_{old} - \\eta \\nabla J(w)$$\n",
    "where $\\eta$ is the **Learning Rate**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Example: Simple Derivative of f(x) = x^2\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "# Check the slope at x = 3\n",
    "x_val = 3\n",
    "print(f\"At x={x_val}, the function value is {f(x_val)}\")\n",
    "print(f\"The slope (gradient) is {df(x_val)}\")\n",
    "print(f\"To minimize, we should move towards: {x_val - 0.1 * df(x_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18386d13",
   "metadata": {},
   "source": [
    "# 2. Optimization Algorithms\n",
    "\n",
    "We use **Gradient Descent** to train models. [cite_start]There are three main variants[cite: 208]:\n",
    "\n",
    "1.  **Batch Gradient Descent:** Uses **all** $n$ samples to calculate the gradient. Precise but slow.\n",
    "    $$g_k = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla l(f_w(x_i), y_i)$$\n",
    "2.  **Stochastic Gradient Descent (SGD):** Uses **one** random sample. Fast but noisy.\n",
    "    $$g_k = \\nabla l(f_w(x_{i_k}), y_{i_k})$$\n",
    "3.  **Mini-batch GD:** Uses a small batch (e.g., 32 samples). The best of both worlds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06693368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Data (House Prices)\n",
    "X = np.array(\n",
    "    [\n",
    "        [1, 2],\n",
    "        [2, 4],\n",
    "    ]\n",
    ")  # Features: [Size, Rooms]\n",
    "y = np.array([5, 10])  # Target: Price\n",
    "\n",
    "# Hyperparameters\n",
    "w = np.array([0.0, 1.0])  # Initialization\n",
    "learning_rate = 0.1\n",
    "n_samples = len(y)\n",
    "\n",
    "# BATCH Gradient Descent Step\n",
    "predictions = X @ w\n",
    "error = predictions - y\n",
    "gradient = (X.T @ error) / n_samples\n",
    "w_batch = w - learning_rate * gradient\n",
    "\n",
    "print(f\"Weights after 1 Batch step: {w_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOCHASTIC Gradient Descent Step\n",
    "# Pick 1 random sample\n",
    "idx = np.random.randint(0, n_samples)\n",
    "x_sample = X[idx]\n",
    "y_sample = y[idx]\n",
    "\n",
    "# Calculate Gradient for just this sample\n",
    "pred_sample = x_sample @ w\n",
    "err_sample = pred_sample - y_sample\n",
    "grad_sample = x_sample * err_sample  # No division by n!\n",
    "\n",
    "w_sgd = w - learning_rate * grad_sample\n",
    "\n",
    "print(f\"Weights after 1 SGD step (Sample {idx}): {w_sgd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cda695",
   "metadata": {},
   "source": [
    "# 3. Convexity and The Hessian\n",
    "\n",
    "A function is **Convex** if it is shaped like a bowl.\n",
    "\n",
    "- **Why it matters:** Convex functions have only one minimum (Global Minimum). We cannot get stuck in local minima.\n",
    "- **The Test:** A twice-differentiable function is convex if its **Hessian Matrix** (second derivatives) is **Positive Semi-Definite (PSD)**.\n",
    "\n",
    "$$H_f(x) \\succeq 0$$\n",
    "\n",
    "This roughly means the \"curvature\" is curving up in all directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f321bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check if a matrix Q is Positive Semi-Definite\n",
    "Q = np.array(\n",
    "    [\n",
    "        [3.0, 1.0],\n",
    "        [1.0, 2.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate Eigenvalues\n",
    "eigvals = np.linalg.eigvals(Q)\n",
    "\n",
    "print(f\"Eigenvalues: {eigvals}\")\n",
    "\n",
    "if np.all(eigvals >= 0):\n",
    "    print(\"The matrix is Positive Semi-Definite -> The function is CONVEX (Safe!)\")\n",
    "else:\n",
    "    print(\n",
    "        \"The matrix is Indefinite -> The function is NON-CONVEX (Danger of Local Minima!)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309a1d2",
   "metadata": {},
   "source": [
    "# 4. Constrained Optimization\n",
    "\n",
    "Sometimes parameters $w$ must stay within a set $C$.\n",
    "We use **Projected Gradient Descent**:\n",
    "\n",
    "1. Take a normal gradient step.\n",
    "2. **Project** (clip) the result back into the valid set $C$.\n",
    "\n",
    "$$x^* = \\text{proj}_C(x - \\eta \\nabla f(x))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Weights must stay between -1 and 1\n",
    "w_temp = np.array([1.5, -2.0, 0.5])  # Calculated weights (some are out of bounds)\n",
    "\n",
    "# Projection Step\n",
    "w_projected = np.clip(w_temp, -1, 1)\n",
    "\n",
    "print(f\"Original: {w_temp}\")\n",
    "print(f\"Projected: {w_projected}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
