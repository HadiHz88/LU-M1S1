{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fafc74e",
   "metadata": {},
   "source": [
    "# ðŸ§ª Binary Classification Metrics Lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf3565f",
   "metadata": {},
   "source": [
    "## Setup and Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba14e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example true labels and predicted probabilities\n",
    "y_true = np.array([1, 0, 1, 0, 1, 0, 1, 0])\n",
    "y_scores = np.array([0.95, 0.90, 0.85, 0.70, 0.60, 0.40, 0.20, 0.10])\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = (y_scores >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d5ea43",
   "metadata": {},
   "source": [
    "## Confusion Matrix and Basic Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"TP={tp}, FP={fp}, FN={fn}, TN={tn}\")\n",
    "\n",
    "# Compute Accuracy, Precision, Recall, and F1\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36349b24",
   "metadata": {},
   "source": [
    "## Try Different Thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.2, 0.4, 0.6, 0.8]\n",
    "results = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_scores >= t).astype(int)\n",
    "    prec = precision_score(y_true, y_pred_t)\n",
    "    rec = recall_score(y_true, y_pred_t)\n",
    "    results.append((t, prec, rec))\n",
    "\n",
    "# TODO: Print precision & recall for each threshold\n",
    "for t, prec, rec in results:\n",
    "    print(f\"Threshold={t:.2f} â†’ Precision={prec:.2f}, Recall={rec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c934f17",
   "metadata": {},
   "source": [
    "## ROC Curve and AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e70eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC metrics\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_true, y_scores)\n",
    "\n",
    "# Compute AUC\n",
    "auc_score = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, marker=\"o\", label=f\"ROC curve (AUC = {auc_score:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Classifier\")\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca8e3ab",
   "metadata": {},
   "source": [
    "## Discussion Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e19dcf",
   "metadata": {},
   "source": [
    "1. What happens to precision and recall when the threshold increases?\n",
    "2. Why might accuracy be misleading in imbalanced datasets?\n",
    "3. What does an AUC value of 0.5, 0.7, and 0.9 imply about model performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b926e982",
   "metadata": {},
   "source": [
    "## Optional Challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5508f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=200, n_features=5, n_informative=3, n_redundant=0, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted probabilities (probability of class 1)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "\n",
    "# Compute and plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, marker=\"o\", label=f\"ROC curve (AUC = {auc_score:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Classifier\")\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
